{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e37cc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint \n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5e4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    ELASTICSEARCH_URL,\n",
    "    INDEX_NAME_HUGGINGFACE,\n",
    "    INDEX_NAME_GEMINI,\n",
    "    INDEX_NAME_OPENAI,\n",
    "    SITEMAP_URL,\n",
    "    HUGGINGFACE_EMBEDDING_MODEL_NAME,\n",
    "    GEMINI_EMBEDDING_MODEL_NAME,\n",
    "    OPENAI_EMBEDDING_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2290d6",
   "metadata": {},
   "source": [
    "**Connect To Elastic Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b15e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(ELASTICSEARCH_URL)\n",
    "print(\"Connected to Elasticsearch!\")\n",
    "\n",
    "client_info = es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5cedd",
   "metadata": {},
   "source": [
    "Crate Index utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name: str):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "        return\n",
    "\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"url\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"content\": {\"type\": \"text\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": 384}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"Index '{index_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df06c6",
   "metadata": {},
   "source": [
    "Extract page content utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7317a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_text(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.extract()\n",
    "\n",
    "        title = soup.title.string.strip() if soup.title else \"Untitled\"\n",
    "        text = re.sub(r\"\\s+\", \" \", soup.get_text(separator=\" \", strip=True))\n",
    "        return title, text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc702522",
   "metadata": {},
   "source": [
    "get_embedder model utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9db096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedder(model_type: str, api_key: Optional[str] = None) -> Callable[[str], list[float]]:\n",
    "    \"\"\"\n",
    "    Initializes the embedding model and returns a function to encode text.\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'HuggingFace', 'Gemini', or 'OpenAI'.\n",
    "        api_key: The necessary API key for the chosen service (if applicable).\n",
    "        \n",
    "    Returns:\n",
    "        A callable function that takes a text string and returns a list of floats (embedding).\n",
    "    \"\"\"\n",
    "    model_type = model_type.lower()\n",
    "\n",
    "    if model_type == \"huggingface\":\n",
    "        # Requires: pip install sentence-transformers\n",
    "        # The specific model (e.g., 'all-MiniLM-L6-v2') can be passed as an argument\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "        except ImportError:\n",
    "            raise ImportError(\"HuggingFace embedder requires 'sentence-transformers'. Please install it.\")\n",
    "        \n",
    "        # Initialize the model (e.g., a common sentence-transformer model)\n",
    "        hf_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "        # Return a lambda that calls the encode method and converts to list\n",
    "        return lambda content: hf_model.encode(content).tolist()\n",
    "\n",
    "    elif model_type == \"gemini\":\n",
    "        # Requires: pip install google-genai\n",
    "        try:\n",
    "            from google import genai\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Gemini embedder requires 'google-genai'. Please install it.\")\n",
    "        \n",
    "        if not api_key:\n",
    "             raise ValueError(\"Gemini embedder requires an API key.\")\n",
    "\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        \n",
    "        def gemini_embed(content: str) -> list[float]:\n",
    "            # Use the 'embedding-001' model for general embeddings\n",
    "            response = client.models.embed_content(\n",
    "                model='models/embedding-001',\n",
    "                content=content,\n",
    "                task_type='RETRIEVAL_DOCUMENT'\n",
    "            )\n",
    "            return response['embedding']\n",
    "\n",
    "        return gemini_embed\n",
    "\n",
    "    elif model_type == \"openai\":\n",
    "        # Requires: pip install openai\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "        except ImportError:\n",
    "            raise ImportError(\"OpenAI embedder requires 'openai'. Please install it.\")\n",
    "        \n",
    "        if not api_key:\n",
    "             raise ValueError(\"OpenAI embedder requires an API key.\")\n",
    "             \n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        def openai_embed(content: str) -> list[float]:\n",
    "            # Use the standard text-embedding-3-small model\n",
    "            response = client.embeddings.create(\n",
    "                input=content,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            # The API returns a list of embeddings, we take the first (and only) one\n",
    "            return response.data[0].embedding\n",
    "\n",
    "        return openai_embed\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Must be 'HuggingFace', 'Gemini', or 'OpenAI'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b27a8b",
   "metadata": {},
   "source": [
    "scrape sitemap and index pages utils func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141 URLs to crawl.\n",
      "['https://www.sisuclinic.com/en-ie/treatments',\n",
      " 'https://www.sisuclinic.com/en-ie/providers',\n",
      " 'https://www.sisuclinic.com/en-ie/faqs',\n",
      " 'https://www.sisuclinic.com/en-ie/locations',\n",
      " 'https://www.sisuclinic.com/en-ie/about-us',\n",
      " 'https://www.sisuclinic.com/en-ie/pricing',\n",
      " 'https://www.sisuclinic.com/en-ie/results',\n",
      " 'https://www.sisuclinic.com/en-ie/accessibility',\n",
      " 'https://www.sisuclinic.com/en-ie/contact-us',\n",
      " 'https://www.sisuclinic.com/en-ie/get-the-look']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def scrape_and_index(model_type: str, index_name: str, api_key: Optional[str] = None):\n",
    "    # 1. Get Embedder Function\n",
    "    try:\n",
    "        # Get the callable function that performs the embedding\n",
    "        embed_content = get_embedder(model_type, api_key=api_key) \n",
    "    except (ImportError, ValueError) as e:\n",
    "        print(f\"FATAL ERROR: Could not initialize embedder. {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Scrape Sitemap\n",
    "    print(f\"Fetching sitemap from: {SITEMAP_URL}\")\n",
    "    sitemap_xml = requests.get(SITEMAP_URL).text\n",
    "    soup = BeautifulSoup(sitemap_xml, \"xml\")\n",
    "\n",
    "    # Filter URLs to the specific path\n",
    "    urls = [loc.text for loc in soup.find_all(\"loc\") if \"/en-ie/\" in loc.text]\n",
    "    \n",
    "    print(f\"Found **{len(urls)}** URLs to crawl.\")\n",
    "    pprint(urls[:5])\n",
    "    print(\"...\")\n",
    "\n",
    "    # 3. Process and Index Pages\n",
    "    total_urls = len(urls)\n",
    "    operations = []\n",
    "    count_operation = 0\n",
    "    batch_size = 50\n",
    "    for url in tqdm(urls, total=total_urls, desc=\"Fetching and Indexing pages...\"):\n",
    "        count_operation += 1\n",
    "        operations.append({\"index\": {\"_index\": index_name}})\n",
    "        try:\n",
    "            # Scrape content\n",
    "            title, content = extract_page_text(url)\n",
    "            \n",
    "            # Skip if content is empty\n",
    "            if not content:\n",
    "                print(f\"â­Skipped ({i}/{total_urls}): Empty content for {url}\")\n",
    "                continue\n",
    "\n",
    "            # Generate Embedding\n",
    "            # Now we use the returned callable function 'embed_content'\n",
    "            embedding = embed_content(content) \n",
    "\n",
    "            # Create Document\n",
    "            doc = {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "\n",
    "            operations.append(doc)\n",
    "\n",
    "            if count_operation >= batch_size:\n",
    "                es.bulk(operations=operations)\n",
    "                count_operation = 0\n",
    "                operations.clear()\n",
    "\n",
    "            # Index Document\n",
    "            es.index(index=index_name, document=doc)\n",
    "            print(f\"Indexed: {url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process/index {url}. Error: {e}\")\n",
    "    \n",
    "    if operations:\n",
    "        es.bulk(operations=operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161896df",
   "metadata": {},
   "source": [
    "**Scrape the pages from the sitemap and use HuggingFace Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3906aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "1 4\n",
      "2 3\n",
      "3 2\n",
      "4 1\n"
     ]
    }
   ],
   "source": [
    "my_list = [5, 4, 3, 2, 1]\n",
    "for i, num in enumerate(my_list):\n",
    "    print(i, num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
